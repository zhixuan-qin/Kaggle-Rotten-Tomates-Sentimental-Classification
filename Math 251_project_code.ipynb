{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sentence Transformer was run in Google Colab under GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from timeit import default_timer\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load training data set\n",
    "from google.colab import files\n",
    "uploaded_train = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load testing data set\n",
    "from google.colab import files\n",
    "uploaded_test = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the datasets\n",
    "train = pd.read_csv(io.BytesIO(uploaded_train['train.tsv']), sep='\\t')\n",
    "test = pd.read_csv(io.BytesIO(uploaded_test['test.tsv']), sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#there are about 26 pretrained models\n",
    "#roberta-large-nli-stsb-mean-tokens - returns 1024 dimentional vector\n",
    "#distilbert-base-nli-stsb-mean-tokens - returns 768 dimentional vector\n",
    "\n",
    "pretrained_model = 'roberta-large-nli-stsb-mean-tokens' #STSb performance is highest\n",
    "model = SentenceTransformer(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_BATCH=128\n",
    "\n",
    "def count_embedd (df):\n",
    "    idx_chunk=list(df.columns).index('Phrase')\n",
    "    embedd_lst = []\n",
    "    for index in range (0, df.shape[0], TRANSFORMER_BATCH):\n",
    "        embedds = model.encode(df.iloc[index:index+TRANSFORMER_BATCH, idx_chunk].values, show_progress_bar=False)\n",
    "        embedd_lst.append(embedds)\n",
    "    return np.concatenate(embedd_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings for TRAIN dataset, 1024 dimentions each\n",
    "start_time = default_timer()\n",
    "train_embedd = count_embedd(train)\n",
    "print(\"Train embeddings: {}: in: {:5.2f}s\".format(train_embedd.shape, default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings for TEST dataset, 1024 dimentions each\n",
    "start_time = default_timer()\n",
    "test_embedd = count_embedd(test)\n",
    "print(\"Test embeddings: {}: in: {:5.2f}s\".format(test_embedd.shape, default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the train_embedd content into local\n",
    "import pickle\n",
    "with open('train_embedd.pickle', 'wb') as f:\n",
    "    pickle.dump(train_embedd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the test_embedd content into local\n",
    "import pickle\n",
    "with open('test_embedd.pickle', 'wb') as f:\n",
    "    pickle.dump(test_embedd, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following work were all run in the local Jupter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.tsv', sep='\\t') # read the original train dataset\n",
    "test = pd.read_csv('test.tsv', sep= '\\t') # read the original test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sentence-transformed training dataset\n",
    "import pickle\n",
    "with open('train_embedd.pickle', 'rb') as f:\n",
    "    train_embedd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sentence-transformed testing dataset\n",
    "import pickle\n",
    "with open('test_embedd.pickle', 'rb') as f:\n",
    "    test_embedd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-transformed training dataset split into \"train\" and \"test\" datasets to balance the number of phrases among classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = train_embedd\n",
    "ytr = train['Sentiment']\n",
    "c0 = Xtr[ytr == 0] # class 0\n",
    "c1 = Xtr[ytr == 1] # class 1\n",
    "c2 = Xtr[ytr == 2] # class 2\n",
    "c3 = Xtr[ytr == 3] # class 3\n",
    "c4 = Xtr[ytr ==4] # class 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## train and test split according to the fix ratio for each class\n",
    "Xtr_0, Xtst_0, ytr_0, ytst_0 = train_test_split(c0, ytr[ytr==0], test_size = 1/3, random_state = 42)\n",
    "Xtr_1, Xtst_1, ytr_1, ytst_1 = train_test_split(c1, ytr[ytr==1], test_size = 2/3, random_state = 42)\n",
    "Xtr_2, Xtst_2, ytr_2, ytst_2 = train_test_split(c2, ytr[ytr==2], test_size = 4/5, random_state = 42)\n",
    "Xtr_3, Xtst_3, ytr_3, ytst_3 = train_test_split(c3, ytr[ytr==3], test_size = 3/4, random_state = 42)\n",
    "Xtr_4, Xtst_4, ytr_4, ytst_4 = train_test_split(c4, ytr[ytr==4], test_size = 1/3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_new = np.concatenate((Xtr_0, Xtr_1, Xtr_2, Xtr_3, Xtr_4), axis = 0) # new training\n",
    "# Xtr_new.shape\n",
    "ytr_new = np.concatenate((ytr_0, ytr_1, ytr_2, ytr_3, ytr_4), axis = 0) # new training labels\n",
    "# ytr_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtst_new = np.concatenate((Xtst_0, Xtst_1, Xtst_2, Xtst_3, Xtst_4), axis = 0) # new testing\n",
    "ytst_new = np.concatenate((ytst_0, ytst_1, ytst_2, ytst_3, ytst_4), axis = 0) # new testing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Ordinal Classification Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalClassifierS1():\n",
    "    \n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for i,y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[y][:,1])\n",
    "            elif y in clfs_predict:\n",
    "                #Vi = Pr(y > Vi-1) - Pr(y > Vi)\n",
    "                 predicted.append(clfs_predict[y-1][:,1] - clfs_predict[y][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[y-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Ordinal Classification Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalClassifierS2():\n",
    "    \n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for i,y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[y][:,1])\n",
    "            elif y in clfs_predict:\n",
    "#                 Vi = (1-Pr(y > Vi-1))*Pr(y > Vi-1)\n",
    "                 predicted.append((1-clfs_predict[y][:,1])*clfs_predict[y-1][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[y-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Ordinal Classification Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalClassifierS3():\n",
    "    \n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for y in self.unique_class:\n",
    "            if y!=max(self.unique_class):\n",
    "                predicted.append(clfs_predict[y][:, 0])\n",
    "            else:\n",
    "                predicted.append([1]*len(X))\n",
    "        return np.vstack(predicted).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        tmp = self.predict_proba(X)\n",
    "        boo = tmp>=0.5\n",
    "        return boo.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from timeit import default_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run classfifiers LDA, QDA, GNB, Logistic Regression, Linear SVM, Random Forest, Adaboost, Neural Networks\n",
    "- standard classification\n",
    "- ordinal classification scenario 1, 2, 3\n",
    "- Ordinal Linear SVM was too time-consuming and we only present standard Linear SVM\n",
    "- Ordinal Adaboost was too time-consuming and we only present standard Adaboost and Ordinal 1 scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LDA\n",
    "clf_r = LinearDiscriminantAnalysis() # standard LDA\n",
    "clf_o1 = OrdinalClassifierS1(clf_r) # ordinal 1 LDA\n",
    "clf_o2 = OrdinalClassifierS2(clf_r) # ordinal 2 LDA\n",
    "clf_o3 = OrdinalClassifierS3(clf_r) # ordinal 3 LDA\n",
    "\n",
    "LDA = [clf_r, clf_o1, clf_o2, clf_o3]\n",
    "\n",
    "time_lda = [] # run time\n",
    "accuracy_lda = [] # accuracy\n",
    "\n",
    "for clf in LDA:\n",
    "    start_time = default_timer()\n",
    "    clf.fit(Xtr_new, ytr_new)\n",
    "    predict = clf.predict(Xtst_new)\n",
    "    accuracy = np.sum(predict == ytst_new)/len(ytst_new)\n",
    "    accuracy_lda.append(accuracy)\n",
    "    time_lda.append(default_timer() - start_time)\n",
    "    print(accuracy_lda)\n",
    "    print(time_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QDA\n",
    "clf_r = QuadraticDiscriminantAnalysis() # standard QDA\n",
    "clf_o1 = OrdinalClassifierS1(clf_r) # ordinal 1 QDA\n",
    "clf_o2 = OrdinalClassifierS2(clf_r) # ordinal 2 QDA\n",
    "clf_o3 = OrdinalClassifierS3(clf_r) # ordinal 3 QDA\n",
    "\n",
    "QDA = [clf_r, clf_o1, clf_o2, clf_o3]\n",
    "\n",
    "time_qda = [] # run time\n",
    "accuracy_qda = [] # accuracy\n",
    "\n",
    "for clf in QDA:\n",
    "    start_time = default_timer()\n",
    "    clf.fit(Xtr_new, ytr_new)\n",
    "    predict = clf.predict(Xtst_new)\n",
    "    accuracy = np.sum(predict == ytst_new)/len(ytst_new)\n",
    "    accuracy_qda.append(accuracy)\n",
    "    time_qda.append(default_timer() - start_time)\n",
    "    print(accuracy_qda)\n",
    "    print(time_qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Guassian Naive Bayes\n",
    "clf_r = GaussianNB() # standard GNB\n",
    "clf_o1 = OrdinalClassifierS1(clf_r) # ordinal 1 GNB\n",
    "clf_o2 = OrdinalClassifierS2(clf_r) # ordinal 2 GNB\n",
    "clf_o3 = OrdinalClassifierS3(clf_r) # ordinal 3 GNB\n",
    "\n",
    "GNB = [clf_r, clf_o1, clf_o2, clf_o3]\n",
    "\n",
    "time_gnb = [] # run time\n",
    "accuracy_gnb = [] # accuracy\n",
    "\n",
    "for clf in GNB:\n",
    "    start_time = default_timer()\n",
    "    clf.fit(Xtr_new, ytr_new)\n",
    "    predict = clf.predict(Xtst_new)\n",
    "    accuracy = np.sum(predict == ytst_new)/len(ytst_new)\n",
    "    accuracy_gnb.append(accuracy)\n",
    "    time_gnb.append(default_timer() - start_time)\n",
    "    print(accuracy_gnb)\n",
    "    print(time_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Guassian Naive Bayes\n",
    "clf_r = GaussianNB() # standard GNB\n",
    "clf_o1 = OrdinalClassifierS1(clf_r) # ordinal 1 GNB\n",
    "clf_o2 = OrdinalClassifierS2(clf_r) # ordinal 2 GNB\n",
    "clf_o3 = OrdinalClassifierS3(clf_r) # ordinal 3 GNB\n",
    "\n",
    "GNB = [clf_r, clf_o1, clf_o2, clf_o3]\n",
    "\n",
    "time_gnb = [] # run time\n",
    "accuracy_gnb = [] # accuracy\n",
    "\n",
    "for clf in GNB:\n",
    "    start_time = default_timer()\n",
    "    clf.fit(Xtr_new, ytr_new)\n",
    "    predict = clf.predict(Xtst_new)\n",
    "    accuracy = np.sum(predict == ytst_new)/len(ytst_new)\n",
    "    accuracy_gnb.append(accuracy)\n",
    "    time_gnb.append(default_timer() - start_time)\n",
    "    print(accuracy_gnb)\n",
    "    print(time_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Logistic Regression\n",
    "clf_r = LogisticRegression(C = 2e-5, solver = 'lbfgs', max_iter=500) # standard LR, C has been tuned and the optimal was used\n",
    "clf_o1 = OrdinalClassifierS1(clf_r) # ordinal 1 LR\n",
    "clf_o2 = OrdinalClassifierS2(clf_r) # ordinal 2 LR\n",
    "clf_o3 = OrdinalClassifierS3(clf_r) # ordinal 3 LR\n",
    "\n",
    "LR = [clf_r, clf_o1, clf_o2, clf_o3]\n",
    "\n",
    "time_lr = [] # run time\n",
    "accuracy_lr = [] # accuracy\n",
    "\n",
    "for clf in LR:\n",
    "    start_time = default_timer()\n",
    "    clf.fit(Xtr_new, ytr_new)\n",
    "    predict = clf.predict(Xtst_new)\n",
    "    accuracy = np.sum(predict == ytst_new)/len(ytst_new)\n",
    "    accuracy_lr.append(accuracy)\n",
    "    time_lr.append(default_timer() - start_time)\n",
    "    print(accuracy_lr)\n",
    "    print(time_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Linear SVM, only standard, ordinal SVM was too time-consuming, we only present the standard version\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "C = 2*np.logspace(-8, 5, 14) # tune regulazation C\n",
    "accuracy_svm= []\n",
    "time_svm = []\n",
    "\n",
    "for i in C:\n",
    "    start_time = default_timer()\n",
    "    clf = OneVsOneClassifier(LinearSVC(C = i))\n",
    "    clf.fit(Xtr_new, ytr_new)\n",
    "    predict = clf.predict(Xtst_new)\n",
    "    accuracy_svm.append(np.sum(predict == ytst_new)/len(ytst_new))\n",
    "    time_svm.append(default_timer() - start_time)\n",
    "    print(accuracy_svm)\n",
    "    print(time_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Networks\n",
    "## regular NN found the highest accuracy = 0.6614927079333042,\n",
    "## learning_rate_init=4.64158883e-02, and alpha = 1.66810054e-01\n",
    "\n",
    "clf_r = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', \n",
    "                        random_state=1, max_iter=1000,\n",
    "                       learning_rate='constant',\n",
    "                       learning_rate_init=4.64158883e-02,\n",
    "                       alpha = 1.66810054e-01,\n",
    "                       batch_size=200) # standard NN, parameters were tuned and the optimals were used\n",
    "clf_o1 = OrdinalClassifierS1(clf_r) # ordinal 1 NN\n",
    "clf_o2 = OrdinalClassifierS2(clf_r) # ordinal 2 NN\n",
    "clf_o3 = OrdinalClassifierS3(clf_r) # ordinal 3 NN\n",
    "\n",
    "NN = [clf_r, clf_o1, clf_o2, clf_o3]\n",
    "\n",
    "time_nn = [] # run time\n",
    "accuracy_nn = [] # accuracy\n",
    "\n",
    "for clf in NN:\n",
    "    start_time = default_timer()\n",
    "    clf.fit(Xtr_new, ytr_new)\n",
    "    predict = clf.predict(Xtst_new)\n",
    "    accuracy = np.sum(predict == ytst_new)/len(ytst_new)\n",
    "    accuracy_nn.append(accuracy)\n",
    "    time_nn.append(default_timer() - start_time)\n",
    "    print(accuracy_nn)\n",
    "    print(time_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "##ordinal RF 3 with 400 number trees found the highest accuracy = 0.6743\n",
    "\n",
    "#standard RF\n",
    "estimators=[50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "score_per_tree=[]\n",
    "for n in estimators:\n",
    "    rf = OrdinalClassifier(RandomForestClassifier(n_estimators=n))\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred=rf.predict(X_test)\n",
    "    score= np.sum(pred == y_test)/len(y_test)\n",
    "    score_per_tree.append(score)\n",
    "    print(score_per_tree)\n",
    "    \n",
    "confusion_matrix(y_test, pred)\n",
    "\n",
    "\n",
    "#ordinal RF 1\n",
    "score_per_tree=[]\n",
    "for n in estimators:\n",
    "    rf = OrdinalClassifierS1(RandomForestClassifier(n_estimators=n))\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred=rf.predict(X_test)\n",
    "    score= np.sum(pred == y_test)/len(y_test)\n",
    "    score_per_tree.append(score)\n",
    "    print(score_per_tree)\n",
    "    \n",
    "#ordinal RF 2\n",
    "score_per_tree=[]\n",
    "for n in estimators:\n",
    "    rf = OrdinalClassifierS2(RandomForestClassifier(n_estimators=n))\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred=rf.predict(X_test)\n",
    "    score= np.sum(pred == y_test)/len(y_test)\n",
    "    score_per_tree.append(score)\n",
    "    print(score_per_tree)\n",
    "\n",
    "#ordinal RF 3\n",
    "score_per_tree=[]\n",
    "for n in estimators:\n",
    "    rf = OrdinalClassifierS3(RandomForestClassifier(n_estimators=n))\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred=rf.predict(X_test)\n",
    "    score= np.sum(pred == y_test)/len(y_test)\n",
    "    score_per_tree.append(score)\n",
    "    print(score_per_tree)\n",
    "    \n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Adaboost, only standard, and limited ordinal Adaboost\n",
    "#### ordinal Adaboost was too time-consuming\n",
    "\n",
    "#standard Adaboost\n",
    "estimators=list(range(100, 501,50))\n",
    "score_per_tree=[]\n",
    "for n in estimators:\n",
    "    ab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(min_samples_leaf=1, max_depth=10),\n",
    "                                              n_estimators=n)\n",
    "    ab.fit(X_train, y_train)\n",
    "    pred=ab.predict(X_test)\n",
    "    score= np.sum(pred == y_test)/len(y_test)\n",
    "    score_per_tree.append(score)\n",
    "    print(score_per_tree)\n",
    "\n",
    "#ordinal Adaboost 1\n",
    "score_per_tree=[]\n",
    "for n in estimators:\n",
    "    ab = OrdinalClassifierS1(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(min_samples_leaf=1, max_depth=10),\n",
    "                                              n_estimators=n))\n",
    "    ab.fit(X_train, y_train)\n",
    "    pred=ab.predict(X_test)\n",
    "    score= np.sum(pred == y_test)/len(y_test)\n",
    "    score_per_tree.append(score)\n",
    "    print(score_per_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration: here we tried to merge the classes (merged 0 and 1, and 3 and 4) and do ordinal3 LR classification on training data with 3 labels. Then we apply binary LR to the 0 and 1 classes, and 3 and 4 classes to evaluate the overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the negative and somewhat negative classes, and somewhat positive and positive classes\n",
    "y_train2=np.array(y_train, copy=True)\n",
    "y_test2=np.array(y_test, copy=True)\n",
    "y_test3=np.array(y_test, copy=True)\n",
    "\n",
    "for i in range(len(y_train2)):\n",
    "    if y_train2[i]==1:\n",
    "        y_train2[i]=0\n",
    "    elif y_train2[i]==2:\n",
    "        y_train2[i]=1\n",
    "    elif y_train2[i]==3 or y_train2[i]==4:\n",
    "        y_train2[i]=2\n",
    "\n",
    "        \n",
    "        \n",
    "for i in range(len(y_test3)):\n",
    "    if y_test3[i]==1:\n",
    "        y_test3[i]=0\n",
    "    elif y_test3[i]==2:\n",
    "        y_test3[i]=1\n",
    "    elif y_test3[i]==3 or y_test3[i]==4:\n",
    "        y_test3[i]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train and fit the ordinal3 LR for merged 3 classes data\n",
    "##we tuned the C and C=2e-4 gave us the best accuracy\n",
    "lr = OrdinalClassifierS3(LogisticRegression(C= 2e-4, solver=\"lbfgs\", max_iter=500))\n",
    "lr.fit(X_train, y_train2)\n",
    "pred=lr.predict(X_test)\n",
    "score= np.sum(pred == y_test3)/len(y_test3)\n",
    "print(score)  #0.7161140197789413, higher than ordinal3 LR on 5 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train the binary LR model for class 0 and 1\n",
    "##again, C=2e-4 gave us the best accuracy\n",
    "lr01=LogisticRegression(C= 2e-4, solver=\"lbfgs\", max_iter=500)\n",
    "index01=y_train2==0\n",
    "lr01.fit(X_train[index01], y_train[index01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train the binary LR model for class 3 and 4\n",
    "##again, C=2e-4 gave us the best accuracy\n",
    "lr34=LogisticRegression(C= 2e-4, solver=\"lbfgs\", max_iter=500)\n",
    "index34=y_train2==2\n",
    "lr34.fit(X_train[index34], y_train[index34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index for corresponding class predictions for test data\n",
    "pred2_index=pred==1\n",
    "pred01_index=pred==0\n",
    "pred34_index=pred==2\n",
    "\n",
    "#fit binary LR on test data\n",
    "pred01=lr01.predict(X_test[pred01_index])\n",
    "pred34=lr34.predict(X_test[pred34_index])\n",
    "\n",
    "#create the final predicted label\n",
    "y_test2[pred2_index]=2\n",
    "y_test2[pred01_index]=pred01\n",
    "y_test2[pred34_index]=pred34\n",
    "\n",
    "#calculate the overall accuracy\n",
    "score= np.sum(y_test2 == y_test)/len(y_test)\n",
    "print(score) #0.6377142345728734, the accuracy is lower than standard LR and all evaluated ordinal LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "test_directory = 'sentiment-analysis-on-movie-reviews/test.tsv/test.tsv'\n",
    "train_directory='sentiment-analysis-on-movie-reviews/train.tsv'\n",
    "test_raw= pd.read_csv(test_directory, sep='\\t')  \n",
    "train_raw=pd.read_csv(train_directory, sep='\\t')  \n",
    "train_label=train_raw[\"Sentiment\"]\n",
    "#drop unecessary columns\n",
    "train_raw.drop(['PhraseId','SentenceId'],inplace = True,axis='columns')\n",
    "\n",
    "\n",
    "#convert sentences to tokenized words\n",
    "for i in range(len(train_raw['Phrase'])):\n",
    "    train_raw['Phrase'][i] = text_to_word_sequence(train_raw['Phrase'][i])\n",
    "    \n",
    "\n",
    "#convert tokenized words to numeric form required for model building\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_raw['Phrase'])\n",
    "\n",
    "train_raw['Phrase'] = tokenizer.texts_to_sequences(train_raw['Phrase'])\n",
    "\n",
    "#convert each tokenized review into an input of the same length = 100 by padding with 0s in the begining\n",
    "max_length = 100\n",
    "train_copy = train_raw['Phrase']\n",
    "train_copy = pad_sequences(train_raw['Phrase'],maxlen=max_length)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X = train_copy\n",
    "y =np.array(train_raw['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resamplep to downsize the training data\n",
    "index0=np.where(y==0)\n",
    "y0=y[index0]\n",
    "x0=X[index0]\n",
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(x0, y0, test_size=0.33, random_state=42)\n",
    "\n",
    "index1=np.where(y==1)\n",
    "y1=y[index1]\n",
    "x1=X[index1]\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(x1, y1, test_size=0.66, random_state=42)\n",
    "\n",
    "index2=np.where(y==2)\n",
    "y2=y[index2]\n",
    "x2=X[index2]\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(x2, y2, test_size=0.8, random_state=42)\n",
    "\n",
    "index3=np.where(y==3)\n",
    "y3=y[index3]\n",
    "x3=X[index3]\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(x3, y3, test_size=0.75, random_state=42)\n",
    "\n",
    "index4=np.where(y==4)\n",
    "y4=y[index4]\n",
    "x4=X[index4]\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(x4, y4, test_size=0.33, random_state=42)\n",
    "\n",
    "#concatenate the new training set labels\n",
    "y_train=np.vstack((y_train_0.reshape((4738,1)),y_train_1.reshape((9272,1))))\n",
    "y_train=np.vstack((y_train,y_train_2.reshape((15916,1))))\n",
    "y_train=np.vstack((y_train,y_train_3.reshape((8231,1))))\n",
    "y_train=np.vstack((y_train,y_train_4.reshape(6168,1)))\n",
    "y_train=y_train.flatten()\n",
    "\n",
    "#concatenate the new setting set data\n",
    "X_train=np.vstack((X_train_0,X_train_1))\n",
    "X_train=np.vstack((X_train,X_train_2))\n",
    "X_train=np.vstack((X_train,X_train_3))\n",
    "X_train=np.vstack((X_train,X_train_4))\n",
    "\n",
    "#concatenate the new testing set labels\n",
    "y_test=np.vstack((y_test_0.reshape((2334,1)),y_test_1.reshape((18001,1))))\n",
    "y_test=np.vstack((y_test,y_test_2.reshape((63666,1))))\n",
    "y_test=np.vstack((y_test,y_test_3.reshape((24696,1))))\n",
    "y_test=np.vstack((y_test,y_test_4.reshape((3038,1))))\n",
    "y_test=y_test.flatten()\n",
    "\n",
    "#concatenate the new testing set data\n",
    "X_test=np.vstack((X_test_0,X_test_1))\n",
    "X_test=np.vstack((X_test,X_test_2))\n",
    "X_test=np.vstack((X_test,X_test_3))\n",
    "X_test=np.vstack((X_test,X_test_4))\n",
    "\n",
    "#turn the label into one hot code\n",
    "y_train_hot = np.zeros((y_train.size, y_train.max()+1))\n",
    "y_train_hot[np.arange(y_train.size),y_train] = 1\n",
    "\n",
    "y_test_hot = np.zeros((y_test.size, y_test.max()+1))\n",
    "y_test_hot[np.arange(y_test.size),y_test] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below model design produced the best accuracy\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_vector_length, \n",
    "                    input_length=max_length))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(5,activation = 'softmax'))\n",
    "model2.compile(loss = 'categorical_crossentropy',\n",
    "                        optimizer = 'adam',\n",
    "                        metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "#fit and test the model\n",
    "train_history=model2.fit(x=X_train,y=y_train_hot,batch_size=64,epochs=10,\n",
    "                         verbose=2,validation_data=(X_test,y_test_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "test_directory = 'sentiment-analysis-on-movie-reviews/test.tsv/test.tsv'\n",
    "train_directory='sentiment-analysis-on-movie-reviews/train.tsv'\n",
    "test_raw= pd.read_csv(test_directory, sep='\\t')  \n",
    "train_raw=pd.read_csv(train_directory, sep='\\t')  \n",
    "train_label=train_raw[\"Sentiment\"]\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punct(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_nopunct\n",
    "train_raw['Text_Clean'] = train_raw['Phrase'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#Tokenize\n",
    "#nltk.download('punkt')\n",
    "tokens = [word_tokenize(sen) for sen in train_raw.Text_Clean]\n",
    "\n",
    "#lower case all tokens\n",
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]\n",
    "\n",
    "train_raw['Text_Final'] = [' '.join(sen) for sen in lower_tokens]\n",
    "train_raw['tokens'] = lower_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add five one hot encoded columns to our data frame, corresponding to the 5 classes\n",
    "neg=[]\n",
    "som_neg=[]\n",
    "neu=[]\n",
    "som_pos=[]\n",
    "pos = []\n",
    "\n",
    "for l in train_raw.Sentiment:\n",
    "    if l == 0:\n",
    "        neg.append(1)\n",
    "        som_neg.append(0)\n",
    "        neu.append(0)\n",
    "        som_pos.append(0)\n",
    "        pos.append(0)\n",
    "    elif l == 1:\n",
    "        neg.append(0)\n",
    "        som_neg.append(1)\n",
    "        neu.append(0)\n",
    "        som_pos.append(0)\n",
    "        pos.append(0)\n",
    "    elif l==2:\n",
    "        neg.append(0)\n",
    "        som_neg.append(0)\n",
    "        neu.append(1)\n",
    "        som_pos.append(0)\n",
    "        pos.append(0)\n",
    "    elif l==3:\n",
    "        neg.append(0)\n",
    "        som_neg.append(0)\n",
    "        neu.append(0)\n",
    "        som_pos.append(1)\n",
    "        pos.append(0)\n",
    "    elif l==4:\n",
    "        neg.append(0)\n",
    "        som_neg.append(0)\n",
    "        neu.append(0)\n",
    "        som_pos.append(0)\n",
    "        pos.append(1)\n",
    "        \n",
    "train_raw['neg']= neg\n",
    "train_raw['som_neg']= som_neg\n",
    "train_raw['neu']=neu\n",
    "train_raw['som_pos']= som_pos\n",
    "train_raw['pos']= pos\n",
    "\n",
    "train_raw = train_raw[['Text_Final', 'tokens', 'Sentiment', 'neg', 'som_neg','neu','som_pos','pos']]\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample to downsize the training data\n",
    "df_train0, df_test0 = train_test_split(\n",
    "  train_raw.loc[train_raw['Sentiment'] == 0],\n",
    "  test_size=0.33,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "df_train1, df_test1 = train_test_split(\n",
    "  train_raw.loc[train_raw['Sentiment'] == 1],\n",
    "  test_size=0.66,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "df_train2, df_test2 = train_test_split(\n",
    "  train_raw.loc[train_raw['Sentiment'] == 2],\n",
    "  test_size=0.8,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "df_train3, df_test3 = train_test_split(\n",
    "  train_raw.loc[train_raw['Sentiment'] == 3],\n",
    "  test_size=0.75,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "df_train4, df_test4 = train_test_split(\n",
    "  train_raw.loc[train_raw['Sentiment'] == 4],\n",
    "  test_size=0.33,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "#concatenating\n",
    "data_train=pd.concat([df_train0, df_train1,df_train2,df_train3,df_train4])\n",
    "data_test=pd.concat([df_test0, df_test1,df_test2,df_test3,df_test4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get maximum training sentence length\n",
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "\n",
    "##get maximum testing sentence length\n",
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load word2vec\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "#get embeddings\n",
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and Pad sequences\n",
    "MAX_SEQUENCE_LENGTH=50\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(train_word_index))\n",
    "train_cnn_data = pad_sequences(training_sequences, \n",
    "                               maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#get the initial embedding weights\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "#determine the running sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will get embeddings from Google News Word2Vec model and save them corresponding to the sequence number \n",
    "#we assigned to each word. If we could not get embeddings we save a random vector for that word.\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "    print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text as a sequence is passed to a CNN. The embeddings matrix is passed to embedding_layer. \n",
    "#Five different filter sizes are applied to each comment, and GlobalMaxPooling1D layers are applied to each layer. \n",
    "#All the outputs are then concatenated. A Dropout layer then Dense then Dropout and then Final Dense layer is applied.\n",
    "#model.summary() will print a brief summary of all the layers with there output shapes.\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    " \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, \n",
    "                        kernel_size=filter_size, \n",
    "                        activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "label_names = ['pos', 'som_pos','neu','som_neg','neg']\n",
    "model = ConvNet(train_embedding_weights, \n",
    "                MAX_SEQUENCE_LENGTH, \n",
    "                len(train_word_index)+1, \n",
    "                EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_train = data_train[label_names].values\n",
    "y_tr=y_train\n",
    "\n",
    "#train CNN\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "hist = model.fit(x_train, \n",
    "                 y_tr, \n",
    "                 epochs=num_epochs, \n",
    "                 validation_split=0.1, \n",
    "                 shuffle=True, \n",
    "                 batch_size=batch_size)\n",
    "\n",
    "#test CNN\n",
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "labels = [0,1,2,3,4]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "sum(data_test.Sentiment==prediction_labels)/len(prediction_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
